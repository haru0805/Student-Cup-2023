# Student-Cup-2023

## 解法
### 前処理
- LightGBM
    - stateはregionから一意に決定されるので、stateの欠損値はregionを参照することで補完
    - size, manufacturerの表記ゆれを修正
    - yearが3000以上のものは存在し得ないはずなので、2000年代に変更
    - cylindersから数値のみを取り出す
- NN
    - 数値の欠損はすべて平均で、カテゴリ変数の欠損は、欠損ということがわかるように補完しました（この辺りは改善できそう）
    - StandardScalerを使用

### 特徴量
- カテゴリ変数についてはtarget encoding
- 数値に関してはそのまま
- size_rankがカテゴリ型であったため、順位を明示したものを追加
    - 'full-size': 4, 'mid-size': 3, 'compact': 2, 'sub-compact': 1
- メーカー名から、どの国の車であるかを追加
- 車の色を、黒系、明るい系などにカテゴライズした特徴量を追加
- 走行距離 / 製造年　を追加
- 緯度経度（geopy）を追加
- condition, fuel, typeに関して、走行距離のaggregationを実施
    - 平均値、中央値
- 上記の平均・中央値と、走行距離の差分を追加

### 学習
 - LightGBMとNNを使用
 - 最終subは以下の2つを提出しました。（最後に選択したファイル2つのスコアが最終評価で一番よかったです）
     - LightGBM（60%）、LightGBM+geopy（30%）、NN（10%）：　43.3074128
     - LightGBM（55%）、LightGBM+geopy（35%）、NN（10%）：　43.3175492

## 他に試したこと
- catBoost
- tabNet
- XGBoost
- カテゴリ変数のOnehot
- aggregationに使用する変数の追加
- 上記のモデルのアンサンブル

## 改善点
- ハイパーパラメータの最適化を行うことができなかった。
- NN系は工夫次第で精度がまだ上がりそうなイメージ。
- 走行距離の重要度が高いので、欠損値を埋めるためのモデルを作ると良いらしい。
- catboostの予測結果を特徴量に追加して、LightGBMで予測すると、かなり精度が上がったらしい。
- 上位の方で、tabNetを使用していた方がいるらしい。
- 実験のコードと提出ファイルが途中でごちゃついて、同じファイルを提出していた（割と終盤に。。。）ので、コード管理を改める。

## 今回勉強になったこと
- 今回初めてcolumn transfomerを使用して便利だった。
- NN系のモデルで、テーブルデータにも精度高く使用できるtabNetというものがあることを知った。
